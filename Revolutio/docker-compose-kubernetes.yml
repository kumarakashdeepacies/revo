version: "3.8"

services:
  redis:
    env_file:
      - ".env_prod"
    image: ${REGISTRY_HOST}/revolutio_kubernetes/redis:5.0.4-stretch
    build: ./redis
    command: redis-server --maxmemory 10gb --maxmemory-policy allkeys-lru --requirepass ${REDIS_PASSWORD}
    restart: on-failure:5
    read_only: true
    stop_grace_period: 3s
    volumes:
      - "redis:/data"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    # ports:
    #   - "6379:6379"

  redis-scheduler:
    env_file:
      - ".env_prod"
    image: ${REGISTRY_HOST}/revolutio_kubernetes/redis:5.0.4-stretch
    build: ./redis
    command: redis-server --maxmemory 10gb --maxmemory-policy allkeys-lru --requirepass ${REDIS_PASSWORD_SCHEDULER}
    restart: on-failure:5
    read_only: true
    stop_grace_period: 3s
    volumes:
      - "redis-scheduler:/data"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    # ports:
    #   - "6479:6379"

  keydb:
    env_file:
      - ".env_prod"
    image: ${REGISTRY_HOST}/revolutio_kubernetes/keydb:latest
    command: keydb-server --server-threads 4 --requirepass ${KEYDB_PASSWORD}
    build: ./keydb
    restart: on-failure:5
    read_only: true
    stop_grace_period: 3s
    volumes:
      - "keydb:/data"
    healthcheck:
      test: ["CMD", "keydb-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    # ports:
    #   - "6478:6379"

  postgres:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/postgres:15-bullseye
    build:
      context: "./postgres"
      dockerfile: ./Dockerfile_update
    restart: on-failure:5
    read_only: true
    tmpfs:
       - /var/run/
    environment:
      - POSTGRES_DB=Platform_DB
      - POSTGRES_USER=revolutio
      - POSTGRES_PASSWORD=supersecretpassword
      - ALLOW_EMPTY_PASSWORD=yes
    volumes:
      - postgresql-data-update:/var/lib/postgresql/data
      - postgresql-backup-update:/backups
      - ./postgres/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
      - ./pgbouncer/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf
    healthcheck:
      test: ["CMD-SHELL", "sh -c 'pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}'"]
      interval: 10s
      timeout: 3s
      retries: 3
    # ports:
    #  - "5432:5432"

  pgbouncer1:
    image: edoburu/pgbouncer:1.17.0
    restart: on-failure:5
    read_only: true
    ports:
      - 5434:5432
    environment:
      - POSTGRESQL_HOST=postgres
      - PGBOUNCER_AUTH_TYPE=trust
      - DB_HOST=postgres
      - DB_USER=revolutio
      - DB_PASSWORD=supersecretpassword
      - POOL_MODE=transaction
      - SERVER_RESET_QUERY=DISCARD ALL
      - IGNORE_STARTUP_PARAMETERS="options"
      - MAX_CLIENT_CONN=100
      - default_pool_size=50
    depends_on:
    - "postgres"
    volumes:
      - ./pgbouncer/userlist.txt:/etc/pgbouncer/userlist.txt
      - pgbouncertmp:/etc/pgbouncer

  postgres-new:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/postgres:15-bullseye
    build:
      context: "./postgres"
      dockerfile: ./Dockerfile_update
    restart: always
    environment:
      - POSTGRES_DB=Platform_DB
      - POSTGRES_USER=revolutio
      - POSTGRES_PASSWORD=supersecretpassword
      - ALLOW_EMPTY_PASSWORD=yes
    volumes:
      - postgresql-data-new:/var/lib/postgresql/data
      - postgresql-backup-new:/backups
      - ./postgres/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
      - ./pgbouncer/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf
    # ports:
    #  - "5432:5432"

  pgbouncer1-new:
    image: edoburu/pgbouncer:1.15.0
    ports:
      - 5432:5432
    environment:
      - POSTGRESQL_HOST=postgres-new
      - PGBOUNCER_AUTH_TYPE=trust
      - DB_HOST=postgres-new
      - DB_USER=revolutio
      - DB_PASSWORD=supersecretpassword
      - POOL_MODE=transaction
      - SERVER_RESET_QUERY=DISCARD ALL
      - IGNORE_STARTUP_PARAMETERS="options"
      - MAX_CLIENT_CONN=100
      - default_pool_size=50
    depends_on:
    - "postgres-new"
    restart: always

  postgres-app:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/postgres:15-bullseye
    build:
      context: "./postgres"
      dockerfile: ./Dockerfile_update
    restart: always
    environment:
      - POSTGRES_DB=App_DB
      - POSTGRES_USER=revolutio
      - POSTGRES_PASSWORD=supersecretpassword
      - ALLOW_EMPTY_PASSWORD=yes
    volumes:
      - postgresql-data-app:/var/lib/postgresql/data
      - postgresql-backup-app:/backups
      - ./postgres/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
      - ./pgbouncer/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf
    # ports:
    #  - "5432:5432"

  pgbouncer1-app:
    image: edoburu/pgbouncer:1.15.0
    ports:
      - 5433:5432
    environment:
      - POSTGRESQL_HOST=postgres-app
      - PGBOUNCER_AUTH_TYPE=trust
      - DB_HOST=postgres-app
      - DB_USER=revolutio
      - DB_PASSWORD=supersecretpassword
      - POOL_MODE=transaction
      - SERVER_RESET_QUERY=DISCARD ALL
      - IGNORE_STARTUP_PARAMETERS="options"
      - MAX_CLIENT_CONN=500
      - default_pool_size=50
    depends_on:
    - "postgres-app"
    restart: always


  # frontend:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/frontend:latest
  #   build: ./frontend
  #   stdin_open: true
  #   tty: true
  #   volumes:
  #     - ./frontend:/app
  #     # One-way volume to use node_modules from inside image
  #     - /app/node_modules
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - NODE_ENV=development
  #   depends_on:
  #     - web
  #   command: npm start

  web:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    command: bash -c 'python3 manage.py collectstatic --no-input; uvicorn config.asgi:application --port 5000 --host 0.0.0.0'
    # command: bash -c 'python3 manage.py collectstatic --no-input; python3 manage.py migrate; uvicorn config.asgi:application --port 5000 --host 0.0.0.0'
    # command: bash -c 'while !</dev/tcp/postgres/5432; do sleep 1; done; python3 manage.py collectstatic --no-input; python3 manage.py compress; python3 manage.py makemigrations; python3 manage.py migrate; python3 manage.py createsuperuser --noinput --username $DJANGO_SUPERUSER_USERNAME --email $DJANGO_SUPERUSER_EMAIL ; uvicorn config.asgi:application --port 5000 --host 0.0.0.0 '
    # command: bash -c 'uvicorn config.asgi:application --port 5000 --host 0.0.0.0'
    depends_on:
      - "redis"
      - "pgbouncer1-new"
      - "redis-scheduler"
    env_file:
      - ".env_prod"
    ports:
      - 5000:5000
    healthcheck:
      test: ['CMD-SHELL', '/bin/bash -c "/opt/revolutio/healthcheck.sh 5000"']
      interval: 200s
      timeout: 30s
      retries: 5
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    stop_grace_period: 3s
    volumes:
      - "${DOCKER_WEB_VOLUME:-./revolutio.conf:/opt/revolutio/revolutio.conf}"
      - static:/opt/revolutio/static
      - ./kore_investment/media:/opt/revolutio/kore_investment/media
      - ./:/opt/revolutio/

  worker:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    entrypoint: ["/bin/sh","-c"]
    command: /rqworker/start
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    shm_size: '6gb'
    env_file:
        - ".env_prod"
    depends_on:
      - "redis"
      - "web"
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50
    volumes:
      - ./:/opt/revolutio/
    init: true

  worker-computation-0:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    command: /rqworker/start-computation-0
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    shm_size: '6gb'
    env_file:
        - ".env_prod"
    depends_on:
      - "redis"
      - "web"
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50
    volumes:
      - ./:/opt/revolutio/
    init: true

  worker-data-upload-0:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    command: /rqworker/start-data-upload-0
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    shm_size: '6gb'
    env_file:
        - ".env_prod"
    depends_on:
      - "redis"
      - "web"
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50
    volumes:
      - ./:/opt/revolutio/
    init: true

  worker-navbar:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    command: /rqworker/start-navbar
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    shm_size: '6gb'
    env_file:
        - ".env_prod"
    depends_on:
      - "redis"
      - "web"
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50
    volumes:
      - ./:/opt/revolutio/
    init: true

  worker-scheduler:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    environment:
      TZ: "UTC"
    command: /rqworker/start-scheduler
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    env_file:
        - ".env_prod"
    depends_on:
      - "redis-scheduler"
      - "redis"
      - "web"
      - "pgbouncer1-new"
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50
    volumes:
      - ./:/opt/revolutio/
    init: true

  worker-scheduler-system:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/revolutio:latest
    build:
      context: "."
      dockerfile: ./Dockerfile-updated-python
    environment:
      TZ: "UTC"
    command: /rqworker/start-scheduler-system
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp:mode=770,size=1000m,uid=70070,gid=101
      - /opt/revolutio/logs:mode=770,size=1000m,uid=70070,gid=101
      - /tmp:mode=770,size=1000m,uid=70070,gid=101
    env_file:
        - ".env_prod"
    depends_on:
      - "redis-scheduler"
      - "redis"
      - "web"
      - "pgbouncer1-new"
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50
    volumes:
      - ./:/opt/revolutio/
    init: true

  rqscheduler:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/rqscheduler:latest
    build: ./rqscheduler
    command: /app/start
    environment:
      TZ: "UTC"
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /opt/temp
    volumes:
      - "${DOCKER_WEB_VOLUME:-./revolutio.conf:/opt/revolutio/revolutio.conf}"
    env_file:
        - ".env_prod"
    depends_on:
      - "redis-scheduler"
      - "web"
      - "worker-scheduler"
    init: true
    healthcheck:
      test: ["CMD", "rq", "info"]
      interval: 5s
      timeout: 30s
      retries: 50

  nginx:
    image: ${REGISTRY_HOST}/revolutio_kubernetes/nginx:latest
    build: ./nginx
    command: nginx -g "daemon off;"
    ports:
      - 8080:8080
    restart: on-failure:5
    read_only: true
    tmpfs:
      - /run:mode=770,size=1k,uid=101,gid=101
      - /var/cache/nginx:mode=770,size=1000k,uid=101,gid=101
    depends_on:
      - web
    healthcheck:
      test: ["CMD", "curl", "--fail", "--insecure", "http://localhost/"]
      interval: 20s
      timeout: 20s
      retries: 5
    volumes:
      - "${DOCKER_NGINX_VOLUME:-./nginx/nginx_dev.conf:/etc/nginx/nginx.conf}"
      - static:/var/www/revolutio/static
      - ./kore_investment/media:/var/www/revolutio/media
      #- "${DOCKER_SSL_CERT:-./nginx/cert.pem:/etc/ssl/fullchain.pem}"
      #- "${DOCKER_SSL_KEY:-./nginx/ssl.key:/etc/ssl/privkey.pem}"

  # zookeeper:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/zookeeper:latest
  #   build:
  #     context: "."
  #     dockerfile: ./hadoop/dockerscripts/Dockerfile_zookeeper
  #   hostname: zookeeper
  #   container_name: zookeeper
  #   ports:
  #     - "2181:2181"
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000

  # broker:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/kafkabroker:latest
  #   build:
  #     context: "."
  #     dockerfile: ./kafka/dockerscripts/Dockerfile_kafkabroker
  #   hostname: broker
  #   container_name: broker
  #   depends_on:
  #     - zookeeper
  #   ports:
  #     - "29092:29092"
  #     - "9092:9092"
  #     - "9101:9101"
  #   environment:
  #     KAFKA_BROKER_ID: 1
  #     KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #     KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
  #     KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  #     KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
  #     KAFKA_JMX_PORT: 9101
  #     KAFKA_JMX_HOSTNAME: localhost

  # schema-registry:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/schemaregistry:latest
  #   build:
  #     context: "."
  #     dockerfile: ./kafka/dockerscripts/Dockerfile_schemaregistry
  #   hostname: schema-registry
  #   container_name: schema-registry
  #   depends_on:
  #     - broker
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
  #     SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'
  #     SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  # connect:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/kafkaconnect:latest
  #   build:
  #     context: "."
  #     dockerfile: ./kafka/dockerscripts/Dockerfile_kafkaconnect
  #   container_name: connect
  #   depends_on:
  #     - broker
  #     - schema-registry
  #   ports:
  #     - "8083:8083"
  #     - "8000:8000"
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
  #     CONNECT_REST_ADVERTISED_HOST_NAME: connect
  #     CONNECT_REST_PORT: 8083
  #     CONNECT_GROUP_ID: compose-connect-group
  #     CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
  #     CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  #     CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
  #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
  #     CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  #     CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  #     CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
  #     CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components/,/usr/share/debezium/"
  #     CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
  #     CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
  #     KAFKA_OPTS: "-agentlib:jdwp=transport=dt_socket,server=y,address=8000,suspend=n"
  #   volumes:
  #     - ./connect:/usr/share/debezium/
  #     - ./python-client/python-scripts/datasets:/usr/share/appdata/

  # namenode:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/hadoop-namenode:latest
  #   build:
  #     context: "."
  #     dockerfile: ./hadoop/dockerscripts/Dockerfile_namenode
  #   container_name: namenode
  #   restart: always
  #   ports:
  #     - 9870:9870
  #     - 9010:9000
  #   volumes:
  #     - hadoop_namenode:/hadoop/dfs/name
  #   environment:
  #     - CLUSTER_NAME=test
  #     - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
  #   env_file:
  #     - ./hadoop/hadoop.env

  # datanode:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/hadoop-datanode:latest
  #   build:
  #     context: "."
  #     dockerfile: ./hadoop/dockerscripts/Dockerfile_datanode
  #   container_name: datanode
  #   restart: always
  #   volumes:
  #     - hadoop_datanode:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #     CORE_CONF_fs_defaultFS: hdfs://namenode:9000
  #   ports:
  #     - "9864:9864"
  #   env_file:
  #     - ./hadoop/hadoop.env

  # resourcemanager:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/hadoop-resourcemanager:latest
  #   build:
  #     context: "."
  #     dockerfile: ./hadoop/dockerscripts/Dockerfile_resourcemanager
  #   container_name: resourcemanager
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
  #   env_file:
  #     - ./hadoop/hadoop.env

  # nodemanager1:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/hadoop-nodemanager:latest
  #   build:
  #     context: "."
  #     dockerfile: ./hadoop/dockerscripts/Dockerfile_nodemanager
  #   container_name: nodemanager
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
  #   env_file:
  #     - ./hadoop/hadoop.env

  # historyserver:
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/hadoop-historyserver:latest
  #   build:
  #     context: "."
  #     dockerfile: ./hadoop/dockerscripts/Dockerfile_historyserver
  #   container_name: historyserver
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
  #   volumes:
  #     - hadoop_historyserver:/hadoop/yarn/timeline
  #   env_file:
  #     - ./hadoop/hadoop.env

  arrowflight:
    env_file:
      - ".env_prod"
    image: ${REGISTRY_HOST}/revolutio/arrowflight
    build: ./arrowflight
    restart: on-failure:5
    stop_grace_period: 3s
    volumes:
      - "data_store:/opt/data_store"
      - "./arrowflight:/app"
    # ports:
    #   - "8880:8815"

  # citusmaster:
  #   container_name: "${COMPOSE_PROJECT_NAME:-citus}_master"
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/citus_master:11.2
  #   build:
  #     context: "./citus"
  #     dockerfile: ./Dockerfile_alpine
  #   ports: ["${COORDINATOR_EXTERNAL_PORT:-5432}:5432"]
  #   labels: ["com.citusdata.role=Master"]
  #   environment:
  #     POSTGRES_USER: "${POSTGRES_USER:-postgres}"
  #     POSTGRES_PASSWORD: "supersecretpassword"
  #     PGUSER: "${POSTGRES_USER:-postgres}"
  #     PGPASSWORD: "supersecretpassword"
  #     POSTGRES_HOST_AUTH_METHOD: "${POSTGRES_HOST_AUTH_METHOD:-trust}"
  #     WALG_AZ_PREFIX: "azure://datastoretesting/walg-folder"
  #     ARCHIVE_MODE: "on"
  #   volumes:
  #     #- citus-master-data-backup:/var/lib/postgresql/data
  #     - citus-master-data:/var/lib/postgresql/data
  #     #- citus-master-data-backup:/pgbackupdata

  # citusworker-1:
  #   container_name: "${COMPOSE_PROJECT_NAME:-citus}_worker_1"
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/citus_master:11.2
  #   build:
  #     context: "./citus"
  #     dockerfile: ./Dockerfile_alpine
  #   ports: ["${COORDINATOR_EXTERNAL_PORT:-5433}:5432"]
  #   labels: ["com.citusdata.role=Master"]
  #   environment:
  #     POSTGRES_USER: "${POSTGRES_USER:-postgres}"
  #     POSTGRES_PASSWORD: "supersecretpassword"
  #     PGUSER: "${POSTGRES_USER:-postgres}"
  #     PGPASSWORD: "supersecretpassword"
  #     POSTGRES_HOST_AUTH_METHOD: "${POSTGRES_HOST_AUTH_METHOD:-trust}"
  #     WALG_AZ_PREFIX: "azure://datastoretesting/walg-worker-1"
  #     ARCHIVE_MODE: "on"
  #   volumes:
  #     #- citus-worker-1-data-backup:/var/lib/postgresql/data
  #     - citus-worker-1-data:/var/lib/postgresql/data
  #     #- citus-worker-1-data-backup:/pgbackupdata

  # citusworker-2:
  #   container_name: "${COMPOSE_PROJECT_NAME:-citus}_worker_2"
  #   image: ${REGISTRY_HOST}/revolutio_kubernetes/citus_master:11.2
  #   build:
  #     context: "./citus"
  #     dockerfile: ./Dockerfile_alpine
  #   ports: ["${COORDINATOR_EXTERNAL_PORT:-5434}:5432"]
  #   labels: ["com.citusdata.role=Master"]
  #   environment:
  #     POSTGRES_USER: "${POSTGRES_USER:-postgres}"
  #     POSTGRES_PASSWORD: "supersecretpassword"
  #     PGUSER: "${POSTGRES_USER:-postgres}"
  #     PGPASSWORD: "supersecretpassword"
  #     POSTGRES_HOST_AUTH_METHOD: "${POSTGRES_HOST_AUTH_METHOD:-trust}"
  #     WALG_AZ_PREFIX: "azure://datastoretesting/walg-worker-2"
  #     ARCHIVE_MODE: "on"
  #   volumes:
  #     #- citus-worker-2-data-backup:/var/lib/postgresql/data
  #     - citus-worker-2-data:/var/lib/postgresql/data
  #     #- citus-worker-2-data-backup:/pgbackupdata

volumes:
  redis:
  redis-scheduler:
  keydb:
  postgresql-data:
  postgresql-backup:
  static:
  user_defined_template:
  user_migration:
  revolutioconf:
  platform_configs:
  media:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  data_store:
  postgresql-data-new:
  postgresql-data-update:
  postgresql-backup-new:
  postgresql-backup-update:
  postgresql-data-app:
  postgresql-backup-app:
  citus-master-data:
  citus-worker-1-data:
  citus-worker-2-data:
  pgbouncertmp: